---
layout: post
title:  "Natural Language Processing Overview"
categories: NLP
---


> 자연어처리(Natural Language Processing)의 큰 그림을 그려보자.



## Background

* [NLP의 기본 절차와 Lexical Analysis](https://ratsgo.github.io/natural%20language%20processing/2017/03/22/lexicon/)


## Segmentation

### Dictionary based Tokenization

### Unsupervised  Segmentation

☞ [Unsupervised  Segmentation]({{site.baseurl}}/{% post_url 2018-07-25-Unsupervised-Segmentation %})

## Matrix

### Term-Document Matrix


### Term-Co-occurence Matrix


### 차원축소

* [SVD와 PCA, 그리고 잠재의미분석(LSA)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)

* [R을 활용한 주성분 분석(principal component analysis) 정리 :: Data 쿡북](http://datacookbook.kr/35)



## Models

### Vector space model

* [idea of statistical semantics](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/)

### Word Weighting

* [Word Weighting(1)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/28/tfidf/)

* [Word Weighting(2)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/14/wordweighting/)

#### Similarity

* [문서 유사도 측정](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/20/docsim/)


### Generative model

* [언어모델(Language Model)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/09/16/LM/)


## Semantics

###  Word Embedding

☞ [Word Embedding]({{site.baseurl}}/{% post_url 2018-08-05-Word-Embedding %})


### Sequence-to-Sequence

* [Sequence-to-Sequence 모델로 뉴스 제목 추출하기](https://ratsgo.github.io/natural%20language%20processing/2017/03/12/s2s/)


## Applications

### Collocations

☞ [Collocations]({{site.baseurl}}/{% post_url 2018-02-03-Collocations %})



### Topic Modeling

* [LSA / PLSA / LDA](https://cs.stanford.edu/~ppasupat/a9online/1140.html)

* [Probabilistic Latent Semantic Analysis](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/05/25/plsa/)

* [Topic Modeling, LDA](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/)


### Comparing Corpuses

* [Comparing Corpuses by Word Use](http://sappingattention.blogspot.com/2011/10/comparing-corpuses-by-word-use.html)


## Scoring

### log-likelihood ratio G2

* [Log-likelihood for comparing texts](http://wordhoard.northwestern.edu/userman/analysis-comparewords.html)


## Visualization

☞ [Data Visualization]({{site.baseurl}}/{% post_url 2018-04-07-data-visualization %})


## Lectures

☞ [자연어처리 강의들]({{site.baseurl}}/{% post_url 2018-08-07-Lectures-about-NLP %})
