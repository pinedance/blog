---
layout: post
title:  "뇌과학으로 본 인공지능의 현주소와 미래"
categories: 보고듣고
---

160407(박문호)뇌과학으로 본 인공지능의 현주소와 미래

IT 기업 EXEM의 주관으로 열린 박문호 박사의 강의였다. 

뇌과학과 인공지능, 특히 Deep Learning의 접점에 대해 들을 수 있었다. 

기억 나는 부분을 중심으로 적어 본다. 

***

# 뇌란?

## 뇌는 세포 배약기이다. 

기본적으로 생물은 세포로 이루어져 있다. Cell, That's all. 세포들이 살아가기 위한 전략의 결과물이다. 뇌는 세포, 특히 neuron을 배양하는 배양기이다. 뇌에서 창발된 '자아'는 뉴런들이 외부로부터 들어온 감각을 통제하기 위해 상정한 가공의 통제센터이다. 따라서 '자아'는 내가 아니다. 

## 뇌는 세계를 탐색하는 Bayes' Machine이다. 

뇌는 Prediction 과 Update를 0.1초 마다 갱신하는 Bayes' Machine이다. 낯선 환경에서는 잠시도 쉬지 못한다. 뇌는 순간순간 (1)기존의 세계를 기준으로 예측을 소나기처럼 뿌리고 (2)감각의 원인을 찾아 확인하며, (3) 기존의 사전지식과 다를 때 세계모델을 바꾼다. 즉 기존의 세계와 달라진 세계를 끊임 없이 비교하여 새로운 세계를 업데이트 해 나간다. (3)의 순간에 우리는 '어?'하게 되고, 이 순간에 학습(일화기억)이 이루어진다. 우리는 우리가 예측했던 것과 다른 것, 즉 '이상한 것'만을 기억한다. 코가 2개인 사람을 봤다고 하자. 평생 기억할 것이다. 

![](http://s5.postimg.org/ny1bn5yef/surprise_05.jpg)

이 과정을 모델링하면 Bayes' theorem이 된다. {@@이 [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)에 대해서는 더 찾아봐야 겠다.}

![Bayes' theorem](https://upload.wikimedia.org/math/d/3/c/d3c7c452b3d01f5415dd9bf15d2ab822.png)

***

# Deep Learning (DL)

## AI  > Machine Learning >  Deep Learning

DL은 Machine Learning의 한 부분이다. 

## DL = RBM + CNN

Deep Learning = restricted Boltzmann machine + Convolutional Neural Networks

## Gibbs sampling과 feature extraction이 열쇠

Gibbs sampling : 원의 넓이를 구하는 공식을 모를 때, 반복된 시행착오에서 얻은 확률을 통해 원의 넓이를 구할 수 있다. 사랑과 같은 문제는 function(함수)로 풀 수 없지만, Gibbs sampling은 거의 모든 문제에 적용할 수 있다. 어렵지만 [관련 포스트](http://parkcu.com/blog/markov-chain-monte-carlo-and-gibbs-sampling/)를 공부해 보자.

***

# 기타

## 인공지능은 정보가 광물(실리콘)으로 스며드는 현상이다. 

지금까지 인류는 생물의 진화에 주목해 왔다. 하지만 그에 못지 않게 광물의 진화사가 있다. 생물은 홀로 진화한 것이 아니며, 광물과 함께 공진화해 왔다. 식물이 자기를 보호하려고 작은 유리가시를 낸다. 풀잎에 살이 베이는 것은 이 때문이다. 생물은 광물을 사용하며, 광물은 생물을 키워낸다. 

인공지능의 발전은 광물 진화의 한 단계로 보아야 한다. 우리가 목도하는 이 모든 경이로운 발전은 실리콘이라는 광물이 존재가 없는 정보를 저장하게 되면서 생겨났다. 인간이라는 생물도 이렇게 광물과의 공진화라는 한 페이지에 서게 되었다.

***

# 개인적인 감상

## 그는 정말 엄청난 학습광

박문호 박사는 이 강의의 청탁을 받고 관련 자료를 모두 찾아본 뒤 1천여장이 넘는 PPT 자료를 만들었다고 한다. 뇌과학과 천체관련 지식이 풍부한 그였지만 인공지능에 대해서는 박식하지 않을 거라고 생각했는데, 걱정을 무색하게 한 강의였다. 그는 정말 엄청난 학습광이다. 

## 핵심은 "Bayes' theorem"

그가 핵심이라고 말한 것은 조건부 확률에 대한 베이스 이론이었다. 관찰된 사건으로 부터 조건부 확률을 얻는 이 방법이 '확률의 세계', '패턴인식의 세계', '뉴런 에너지의 세계'를 연결하는 연결고리라는 것이다. 기억은 "뉴런 결합의 에너지가 가장 낮은 상태"이며, 동시에 "조건부 확률이 가장 높은 상태"라고 설명할 수 있다고 했다. 따라서 AI의 추론 과정은 이 가장 낮은 에너지 상태, 혹은 가장 높은 조건부 확률의 상태를 찾는 최적화 문제라고 할 수 있다. 지금까지 AI가 실패했던 것은 이 최적화에 너무 많은 연산이 들어갔기 때문이다. 확률 계산을 위해 모든 경우의 수를 다 찾아 연산하다 보니 시간이 많이 걸렸고 더군다나 over fitting issue를 피하기 어려웠다. 학습에 시간도 많이 걸리고 작은 변화가 생겨도 다르다고 인식하게 되었다. DL의 성공요인은 근사치를 통해 이 두가지 문제를 해결했다는 점이다. 모든 경우의 수를 다 연산하는 것이 아니라 Convolutional Neural Networks를 통해 이를 대폭 줄였다는 점이다. (공부자료: [deep-learning-nutshell-core-concepts](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/))

![](http://s5.postimg.org/k6rr0ukpz/Convolution_schematic.gif)(그림출처: [deep-learning-nutshell-core-concepts](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/))

## Deep = 玄

Deep Learning은 초기에 deep belief net으로 발표되었다고 한다. 신경망구조를 다층으로 쌓은 것이 기본 아이디어로, 2개 이상의 다층으로 이루어진 신경망 속에서 무슨 일이 일어나는지 알 수 없다고 하여 'deep'이란 형용사가 붙었다고 한다. 이런 의미에 적확한 것이 '眩'이다. 가물가물하여 그 깊이를 알 수 없는 오묘한 것을 眩이라고 한다. 

## 명상과 잡념

명상을 할 때 잡념을 없애라고 한다. 하지만 실재로 해보면 잡념이 없어지기는 커녕 더 많이 생긴다. 당연하다. 뇌는 생각하는 기관이다. 생각이 뇌가 존재하는 이유이다. 박문호에 따르면 이 생각의 실체는 '지각을 기반으로 예측', '감각과의 비교', '그리고 지각의 갱신'이다. 따라서 잡념을 없애려는 시도는 당연히 실패로 끝날 수 밖에 없다. '왜 잡념이 없어지지 않을까?', '어떻게 하면 잡념을 없앨 수 있을까'하는 질문이나 '잡념을 없애야지!'하는 의도를 가지면 다시 뇌에 생각꺼리를 주게 된다. 그러면 영원히 잡념에서 벗어날 수 없다. 
잡념은 더이상 낯설지 않은 상태에서 사라진다. 더이상 예측할 것도 비교할 것도 갱신할 것도 없는 상태가 되면 뇌는 자연히 할 일이 없어 쉬게 된다. 명상을 하기 위해서는 생각이 모두 '휘발'될 때까지 기다려야 한다. 그래서 "그냥 편안히 내비두라"고 하는데, 또 이 말을 낯설게 받아들이여 이것에 대해 생각하게 되면 잡념이 휘발되는 속도보다 잡념이 생기는 속도가 더 빨라 잡념을 걷어낼 수 없게 된다. 미궁에 빠지게 된다. 그래서 잡으려면 잡을 수 없고 놓았을 때 잡을 수 있다고 했나 보다.  

## '인간'이라는 현상, 그리고 '나'라는 현상

예전에 recursive가 자아를 이해하는 열쇠일 수 있다는 이야기를 들었다. 이번 강의의 주제와 연결되는 부분이다. 인간을 '스스로 관찰하는 기관'으로 보았을 때, 스스로 관찰하는 자기 자신을 관찰하게 되는 재귀적 상태에 놓이게 된다. 관찰 대상이 변하게 되면 재귀적으로 관찰결과가 바뀌게 되고, 관찰결과를 바라보는 관찰자가 바뀌게 된다. 이러한 갱신 과정이 박문호 박사가 베이즈 이론을 가지고 설명한 예측과 관찰과 갱신의 의미일 것이다. 

